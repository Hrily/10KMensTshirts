{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "import time, datetime\n",
    "import requests\n",
    "import sys, threading, subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_datetime():\n",
    "    return datetime.datetime.utcfromtimestamp(time.time()).strftime('%Y-%m-%d_%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mens Tshirt URL\n",
    "url = \"http://www.jabong.com/men/clothing/polos-tshirts/?sort=popularity&dir=desc&limit=1000&page=\"\n",
    "# Open csv file for writing\n",
    "info_csv = open('info_' + get_datetime() + '.csv', 'w')\n",
    "info_writer = csv.writer(info_csv, delimiter=',')\n",
    "# Write headers\n",
    "info_writer.writerow(['ID', 'Title', 'Original Price', 'Discount Price', 'Product URL', 'Product Image URL'])\n",
    "info_writer.writerow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Thread mutex for global resources\n",
    "mutex = threading.Lock()\n",
    "# Info List\n",
    "info_list = []\n",
    "# Image url List\n",
    "img_urls = []\n",
    "\n",
    "# Class for Fetch thread\n",
    "class fetch_thread(threading.Thread):\n",
    "    # Page to be fetched by this thread\n",
    "    page = 0\n",
    "    def __init__(self, p):\n",
    "        super(fetch_thread, self).__init__()\n",
    "        self.page = p\n",
    "    \n",
    "    def run(self):\n",
    "        global url, mutex, info_list, proc_time, img_urls\n",
    "        # Get contents of Page #page\n",
    "        try:\n",
    "            content = requests.get(url + str(self.page))\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # When failed to connect\n",
    "            print \"Exception\"\n",
    "            print e\n",
    "            return\n",
    "        # Cook soup of contents\n",
    "        soup = BeautifulSoup(content.text, 'lxml')\n",
    "        # Get all products\n",
    "        shirts = soup.find_all('div', class_='product-tile')\n",
    "        for shirt in shirts:\n",
    "            # Skip dummy product\n",
    "            if shirt.a['href'] == u'#':\n",
    "                continue\n",
    "            sid = shirt['data-product-id']\n",
    "            title = shirt.find('div', class_='product-info').div.text\n",
    "            prices = shirt.find_all('span', class_='standard-price')\n",
    "            original_price = prices[0].string\n",
    "            # If there is no discount\n",
    "            discount_price = original_price\n",
    "            # Else if there is a discount\n",
    "            if len(prices) > 1:\n",
    "                discount_price = shirt.find_all('span', class_='standard-price')[1].string\n",
    "            surl = shirt.a['href']\n",
    "            js = json.loads(shirt.img['data-img-config'])\n",
    "            img_url = js['base_path'] + js['500']\n",
    "            pos = shirt.a['data-pos']\n",
    "            row = [pos, sid, title, original_price, discount_price, surl, img_url]\n",
    "            # Append shirt info to info list\n",
    "            # Also append image url to url list\n",
    "            mutex.acquire()\n",
    "            info_list.append(row)\n",
    "            img_urls.append(img_url)\n",
    "            mutex.release()\n",
    "        print 'Wrote page ' + str(self.page)+ ' - ' + str(len(shirts)) + ' items'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Execution time : 0:00:51.791939\n"
     ]
    }
   ],
   "source": [
    "# Start fetching pages\n",
    "start_time = time.time()\n",
    "threads = [ fetch_thread(i+1) for i in range(11) ]\n",
    "[t.start() for t in threads]\n",
    "[t.join() for t in threads]\n",
    "\n",
    "# Sort shirts according to popularity\n",
    "def getKey(item):\n",
    "    return int(item[0])\n",
    "info_list = sorted(info_list, key=getKey)\n",
    "\n",
    "# Write to csv file\n",
    "for row in info_list:\n",
    "    info_writer.writerow([ unicode(s).encode(\"utf-8\") for s in row[1:] ])\n",
    "    info_csv.flush()\n",
    "\n",
    "# Print details\n",
    "exec_time = time.time() - start_time\n",
    "exec_time = str(datetime.timedelta(seconds=exec_time))\n",
    "print \"\\nTotal Execution time : \"  + exec_time\n",
    "# Close csv file\n",
    "info_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_images(imgs):\n",
    "    for img in imgs:\n",
    "        subprocess.call('wget -q -P images \"' + img + '\"',  shell=True)\n",
    "start_time = time.time()\n",
    "get_images(img_urls)\n",
    "fetch_time = time.time() - start_time\n",
    "fetch_time = str(datetime.timedelta(seconds=fetch_time))\n",
    "print \"Images fetched in \" + fetch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
